type: session
title: "False confidence in cross-validation for machine learning"
slug: "false-confidence-in-cross-validation-for-machine-learning"
url: "talks/false-confidence-in-cross-validation-for-machine-learning/index.html"
body_class_hack: talks
---

### Calvin Giles

Cross-validation can be performed in many ways - KFold, Leave-out-out, bootstrapping, repeated etc.. Which one should you choose and will it give you an unbiased estimate of how your model will perform in the wild?
I will answer these questions using a variety of simulated datasets that make and break different assumptions.